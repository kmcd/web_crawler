= web_crawler

A prototype of a large scale web crawler written in Ruby.

== Features

non blocking
dns caching
single process, ie does not share DNS cache
Canonical URL
Store page directory

== Architecture

== Usage

# Must specify a start point
# Optionally limit to a domain

crawler = Crawler.new 'http://google.com'
crawler.start

# Fetches page
# Resolve DNS from cache
# 
# Extracts all URLS
# Forms canonoical representation of all URLs
# 

# Log output
# Stores downloaded files in .ruby_crawler/$URL


== Note on Patches/Pull Requests
 
* Fork the project.
* Make your feature addition or bug fix.
* Add tests for it. This is important so I don't break it in a
  future version unintentionally.
* Commit, do not mess with rakefile, version, or history.
  (if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull)
* Send me a pull request. Bonus points for topic branches.

== Copyright

Copyright (c) 2011 Keith Mc Donnell. See LICENSE for details.
